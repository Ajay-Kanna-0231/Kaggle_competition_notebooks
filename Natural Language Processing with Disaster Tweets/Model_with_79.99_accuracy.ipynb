{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269c2395",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:37.583746Z",
     "iopub.status.busy": "2024-02-27T16:10:37.583360Z",
     "iopub.status.idle": "2024-02-27T16:10:38.616467Z",
     "shell.execute_reply": "2024-02-27T16:10:38.615214Z"
    },
    "papermill": {
     "duration": 1.044173,
     "end_time": "2024-02-27T16:10:38.619069",
     "exception": false,
     "start_time": "2024-02-27T16:10:37.574896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb771e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:38.633234Z",
     "iopub.status.busy": "2024-02-27T16:10:38.632712Z",
     "iopub.status.idle": "2024-02-27T16:10:38.731022Z",
     "shell.execute_reply": "2024-02-27T16:10:38.729173Z"
    },
    "papermill": {
     "duration": 0.108069,
     "end_time": "2024-02-27T16:10:38.733500",
     "exception": false,
     "start_time": "2024-02-27T16:10:38.625431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          id keyword location  \\\n",
      "0         1     NaN      NaN   \n",
      "1         4     NaN      NaN   \n",
      "2         5     NaN      NaN   \n",
      "3         6     NaN      NaN   \n",
      "4         7     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
      "1                Forest fire near La Ronge Sask. Canada       1  \n",
      "2     All residents asked to 'shelter in place' are ...       1  \n",
      "3     13,000 people receive #wildfires evacuation or...       1  \n",
      "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  Two giant cranes holding a bridge collapse int...       1  \n",
      "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
      "7611  Police investigating after an e-bike collided ...       1  \n",
      "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
      "\n",
      "[7613 rows x 5 columns]>\n",
      "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
      "<bound method NDFrame.head of          id keyword location  \\\n",
      "0         0     NaN      NaN   \n",
      "1         2     NaN      NaN   \n",
      "2         3     NaN      NaN   \n",
      "3         9     NaN      NaN   \n",
      "4        11     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "3258  10861     NaN      NaN   \n",
      "3259  10865     NaN      NaN   \n",
      "3260  10868     NaN      NaN   \n",
      "3261  10874     NaN      NaN   \n",
      "3262  10875     NaN      NaN   \n",
      "\n",
      "                                                   text  \n",
      "0                    Just happened a terrible car crash  \n",
      "1     Heard about #earthquake is different cities, s...  \n",
      "2     there is a forest fire at spot pond, geese are...  \n",
      "3              Apocalypse lighting. #Spokane #wildfires  \n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
      "...                                                 ...  \n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
      "3259  Storm in RI worse than last hurricane. My city...  \n",
      "3260  Green Line derailment in Chicago http://t.co/U...  \n",
      "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
      "3262  #CityofCalgary has activated its Municipal Eme...  \n",
      "\n",
      "[3263 rows x 4 columns]>\n",
      "Index(['id', 'keyword', 'location', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "print(train_df.head)\n",
    "print(train_df.columns)\n",
    "print(test_df.head)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4dbdc98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:38.748758Z",
     "iopub.status.busy": "2024-02-27T16:10:38.747698Z",
     "iopub.status.idle": "2024-02-27T16:10:40.546319Z",
     "shell.execute_reply": "2024-02-27T16:10:40.545387Z"
    },
    "papermill": {
     "duration": 1.808689,
     "end_time": "2024-02-27T16:10:40.548715",
     "exception": false,
     "start_time": "2024-02-27T16:10:38.740026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 54)\n",
      "  (0, 34)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 5)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 41)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 51)\t1\n",
      "  :\t:\n",
      "  (2, 32)\t1\n",
      "  (2, 15)\t1\n",
      "  (3, 21)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 32)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 35)\t1\n",
      "  (3, 40)\t1\n",
      "  (3, 53)\t1\n",
      "  (3, 10)\t1\n",
      "  (4, 50)\t1\n",
      "  (4, 53)\t1\n",
      "  (4, 23)\t1\n",
      "  (4, 20)\t1\n",
      "  (4, 46)\t1\n",
      "  (4, 36)\t1\n",
      "  (4, 19)\t2\n",
      "  (4, 43)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 48)\t1\n",
      "  (4, 38)\t1\n",
      "  (4, 22)\t1\n",
      "  (4, 45)\t1\n",
      "First document: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "First tweet data out the first 5: (1, 54) here 54 is the no of distinct words found across the 5 documents\n",
      "[[0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]] 13\n",
      "Second document: Forest fire near La Ronge Sask. Canada\n",
      "Second tweet data out the first 5: (1, 54) here 54 is the no of distinct words found across the 5 documents\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]] 7\n",
      "Third document: All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "Third tweet data out the first 5: (1, 54) here 54 is the no of distinct words found across the 5 documents\n",
      "[[0 0 0 1 0 2 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 2 0 0 0 0 0 1 1 0 1 1 1 1 0 0\n",
      "  0 2 0 0 0 1 0 0 0 0 0 2 0 0 0 1 0 0]] 22\n",
      "Fourth document: 13,000 people receive #wildfires evacuation orders in California \n",
      "Fourth tweet data out the first 5: (1, 54) here 54 is the no of distinct words found across the 5 documents\n",
      "[[1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]] 9\n",
      "Fifth document: Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "Fifth tweet data out the first 5: (1, 54) here 54 is the no of distinct words found across the 5 documents\n",
      "[[0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1]] 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])\n",
    "print(example_train_vectors.shape)\n",
    "print(example_train_vectors)\n",
    "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
    "print(\"First document:\", train_df.text[0])\n",
    "print(\"First tweet data out the first 5:\", example_train_vectors[0].todense().shape, \"here 54 is the no of distinct words found across the 5 documents\") \n",
    "print(example_train_vectors[0].todense(),  example_train_vectors[0].todense().sum())\n",
    "print(\"Second document:\", train_df.text[1])\n",
    "print(\"Second tweet data out the first 5:\", example_train_vectors[1].todense().shape, \"here 54 is the no of distinct words found across the 5 documents\")\n",
    "print(example_train_vectors[1].todense(), example_train_vectors[1].todense().sum())\n",
    "print(\"Third document:\", train_df.text[2])\n",
    "print(\"Third tweet data out the first 5:\", example_train_vectors[2].todense().shape, \"here 54 is the no of distinct words found across the 5 documents\")\n",
    "print(example_train_vectors[2].todense(),  example_train_vectors[2].todense().sum())\n",
    "print(\"Fourth document:\", train_df.text[3])\n",
    "print(\"Fourth tweet data out the first 5:\", example_train_vectors[3].todense().shape, \"here 54 is the no of distinct words found across the 5 documents\")\n",
    "print(example_train_vectors[3].todense(),  example_train_vectors[3].todense().sum())\n",
    "print(\"Fifth document:\", train_df.text[4])\n",
    "print(\"Fifth tweet data out the first 5:\", example_train_vectors[4].todense().shape, \"here 54 is the no of distinct words found across the 5 documents\")\n",
    "print(example_train_vectors[4].todense(),  example_train_vectors[4].todense().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c043eb27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:40.564337Z",
     "iopub.status.busy": "2024-02-27T16:10:40.563647Z",
     "iopub.status.idle": "2024-02-27T16:10:41.001974Z",
     "shell.execute_reply": "2024-02-27T16:10:41.000625Z"
    },
    "papermill": {
     "duration": 0.4503,
     "end_time": "2024-02-27T16:10:41.005977",
     "exception": false,
     "start_time": "2024-02-27T16:10:40.555677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 21637)\n",
      "  (0, 14003)\t1\n",
      "  (0, 5490)\t1\n",
      "  (0, 2192)\t1\n",
      "  (0, 18669)\t1\n",
      "  (0, 15678)\t1\n",
      "  (0, 13681)\t1\n",
      "  (0, 18777)\t1\n",
      "  (0, 6379)\t1\n",
      "  (0, 12141)\t1\n",
      "  (0, 1852)\t1\n",
      "  (0, 7661)\t1\n",
      "  (0, 19774)\t1\n",
      "  (0, 1851)\t1\n",
      "  (1, 7652)\t1\n",
      "  (1, 7439)\t1\n",
      "  (1, 13122)\t1\n",
      "  (1, 11091)\t1\n",
      "  (1, 16266)\t1\n",
      "  (1, 16611)\t1\n",
      "  (1, 3843)\t1\n",
      "  (2, 2192)\t2\n",
      "  (2, 1851)\t1\n",
      "  (2, 15940)\t1\n",
      "  (2, 2312)\t1\n",
      "  (2, 18971)\t1\n",
      "  :\t:\n",
      "  (7611, 11518)\t1\n",
      "  (7611, 16895)\t1\n",
      "  (7611, 1670)\t1\n",
      "  (7611, 9967)\t1\n",
      "  (7611, 18814)\t1\n",
      "  (7611, 13361)\t1\n",
      "  (7611, 3047)\t2\n",
      "  (7611, 4589)\t1\n",
      "  (7611, 14783)\t1\n",
      "  (7611, 18088)\t1\n",
      "  (7611, 9841)\t1\n",
      "  (7612, 18669)\t1\n",
      "  (7612, 3698)\t1\n",
      "  (7612, 3797)\t1\n",
      "  (7612, 9304)\t1\n",
      "  (7612, 4517)\t1\n",
      "  (7612, 13195)\t1\n",
      "  (7612, 12698)\t1\n",
      "  (7612, 11187)\t1\n",
      "  (7612, 13391)\t1\n",
      "  (7612, 1434)\t1\n",
      "  (7612, 20606)\t1\n",
      "  (7612, 9170)\t1\n",
      "  (7612, 15606)\t1\n",
      "  (7612, 21262)\t1\n",
      "-------------------------------------------\n",
      "  (0, 3904)\t1\n",
      "  (0, 4977)\t1\n",
      "  (0, 8753)\t1\n",
      "  (0, 10550)\t1\n",
      "  (0, 18591)\t1\n",
      "  (1, 1464)\t1\n",
      "  (1, 4357)\t1\n",
      "  (1, 5783)\t1\n",
      "  (1, 6379)\t1\n",
      "  (1, 6934)\t1\n",
      "  (1, 8893)\t1\n",
      "  (1, 10043)\t1\n",
      "  (1, 16503)\t1\n",
      "  (1, 17843)\t1\n",
      "  (2, 1539)\t1\n",
      "  (2, 1851)\t1\n",
      "  (2, 2192)\t1\n",
      "  (2, 2358)\t1\n",
      "  (2, 3864)\t1\n",
      "  (2, 7439)\t1\n",
      "  (2, 7524)\t1\n",
      "  (2, 7652)\t1\n",
      "  (2, 10043)\t1\n",
      "  (2, 14751)\t1\n",
      "  (2, 16637)\t1\n",
      "  :\t:\n",
      "  (3259, 21164)\t1\n",
      "  (3260, 4232)\t1\n",
      "  (3260, 4517)\t1\n",
      "  (3260, 5624)\t1\n",
      "  (3260, 8448)\t1\n",
      "  (3260, 9304)\t1\n",
      "  (3260, 9718)\t1\n",
      "  (3260, 11479)\t1\n",
      "  (3261, 658)\t1\n",
      "  (3261, 4517)\t1\n",
      "  (3261, 8840)\t1\n",
      "  (3261, 9304)\t1\n",
      "  (3261, 9396)\t1\n",
      "  (3261, 10070)\t1\n",
      "  (3261, 12239)\t1\n",
      "  (3261, 14021)\t1\n",
      "  (3261, 20439)\t1\n",
      "  (3262, 1550)\t1\n",
      "  (3262, 4364)\t1\n",
      "  (3262, 6609)\t1\n",
      "  (3262, 8801)\t1\n",
      "  (3262, 10084)\t1\n",
      "  (3262, 12866)\t1\n",
      "  (3262, 14619)\t1\n",
      "  (3262, 21363)\t1\n",
      "(3263, 21637)\n"
     ]
    }
   ],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
    "print(train_vectors.shape)\n",
    "print(train_vectors)\n",
    "print(\"-------------------------------------------\")\n",
    "print(test_vectors)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f51d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:41.022799Z",
     "iopub.status.busy": "2024-02-27T16:10:41.022354Z",
     "iopub.status.idle": "2024-02-27T16:10:41.262705Z",
     "shell.execute_reply": "2024-02-27T16:10:41.261510Z"
    },
    "papermill": {
     "duration": 0.25241,
     "end_time": "2024-02-27T16:10:41.266244",
     "exception": false,
     "start_time": "2024-02-27T16:10:41.013834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "clf = linear_model.RidgeClassifier()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_1 = LogisticRegression(C=1.0)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_2 = MultinomialNB()\n",
    "from sklearn.svm import SVC\n",
    "clf_3 = SVC(C=1.0, probability=True)\n",
    "import xgboost as xgb\n",
    "clf_4 = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7e439a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:41.281816Z",
     "iopub.status.busy": "2024-02-27T16:10:41.281351Z",
     "iopub.status.idle": "2024-02-27T16:10:42.083584Z",
     "shell.execute_reply": "2024-02-27T16:10:42.082406Z"
    },
    "papermill": {
     "duration": 0.814388,
     "end_time": "2024-02-27T16:10:42.087799",
     "exception": false,
     "start_time": "2024-02-27T16:10:41.273411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59453669, 0.5642787 , 0.64082434])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf8990b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:42.142659Z",
     "iopub.status.busy": "2024-02-27T16:10:42.141844Z",
     "iopub.status.idle": "2024-02-27T16:10:46.436595Z",
     "shell.execute_reply": "2024-02-27T16:10:46.434863Z"
    },
    "papermill": {
     "duration": 4.330549,
     "end_time": "2024-02-27T16:10:46.440759",
     "exception": false,
     "start_time": "2024-02-27T16:10:42.110210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.6387547 , 0.61347869, 0.68350669])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf_1, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a24320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:46.523377Z",
     "iopub.status.busy": "2024-02-27T16:10:46.522647Z",
     "iopub.status.idle": "2024-02-27T16:10:46.565111Z",
     "shell.execute_reply": "2024-02-27T16:10:46.564173Z"
    },
    "papermill": {
     "duration": 0.096285,
     "end_time": "2024-02-27T16:10:46.567696",
     "exception": false,
     "start_time": "2024-02-27T16:10:46.471411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66763006, 0.6557971 , 0.72431507])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf_2, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1e4f1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:10:46.584741Z",
     "iopub.status.busy": "2024-02-27T16:10:46.584124Z",
     "iopub.status.idle": "2024-02-27T16:11:25.849624Z",
     "shell.execute_reply": "2024-02-27T16:11:25.848508Z"
    },
    "papermill": {
     "duration": 39.283621,
     "end_time": "2024-02-27T16:11:25.859006",
     "exception": false,
     "start_time": "2024-02-27T16:10:46.575385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6012843 , 0.54524362, 0.60536398])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "svd = decomposition.TruncatedSVD(n_components = 120)\n",
    "svd.fit(train_vectors)\n",
    "train_svd = svd.transform(train_vectors)\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(train_svd)\n",
    "train_svd_scl = scl.transform(train_svd)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf_3, train_svd_scl, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41dad16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:11:25.874978Z",
     "iopub.status.busy": "2024-02-27T16:11:25.874529Z",
     "iopub.status.idle": "2024-02-27T16:11:35.764084Z",
     "shell.execute_reply": "2024-02-27T16:11:35.762858Z"
    },
    "papermill": {
     "duration": 9.900644,
     "end_time": "2024-02-27T16:11:35.766754",
     "exception": false,
     "start_time": "2024-02-27T16:11:25.866110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59199071, 0.54358419, 0.5979926 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf_4, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06970697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:11:35.784773Z",
     "iopub.status.busy": "2024-02-27T16:11:35.784317Z",
     "iopub.status.idle": "2024-02-27T16:11:36.857314Z",
     "shell.execute_reply": "2024-02-27T16:11:36.854302Z"
    },
    "papermill": {
     "duration": 1.086346,
     "end_time": "2024-02-27T16:11:36.861733",
     "exception": false,
     "start_time": "2024-02-27T16:11:35.775387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_1.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231f6684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:11:36.943826Z",
     "iopub.status.busy": "2024-02-27T16:11:36.942971Z",
     "iopub.status.idle": "2024-02-27T16:11:36.961608Z",
     "shell.execute_reply": "2024-02-27T16:11:36.959616Z"
    },
    "papermill": {
     "duration": 0.056362,
     "end_time": "2024-02-27T16:11:36.964280",
     "exception": false,
     "start_time": "2024-02-27T16:11:36.907918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281b03f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:11:36.982659Z",
     "iopub.status.busy": "2024-02-27T16:11:36.981415Z",
     "iopub.status.idle": "2024-02-27T16:11:36.988898Z",
     "shell.execute_reply": "2024-02-27T16:11:36.988002Z"
    },
    "papermill": {
     "duration": 0.018893,
     "end_time": "2024-02-27T16:11:36.991152",
     "exception": false,
     "start_time": "2024-02-27T16:11:36.972259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = clf_1.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c965949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:11:37.009475Z",
     "iopub.status.busy": "2024-02-27T16:11:37.008840Z",
     "iopub.status.idle": "2024-02-27T16:11:37.021243Z",
     "shell.execute_reply": "2024-02-27T16:11:37.019832Z"
    },
    "papermill": {
     "duration": 0.024514,
     "end_time": "2024-02-27T16:11:37.023907",
     "exception": false,
     "start_time": "2024-02-27T16:11:36.999393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b5f731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T16:11:37.041864Z",
     "iopub.status.busy": "2024-02-27T16:11:37.041440Z",
     "iopub.status.idle": "2024-02-27T16:11:37.054528Z",
     "shell.execute_reply": "2024-02-27T16:11:37.053423Z"
    },
    "papermill": {
     "duration": 0.025301,
     "end_time": "2024-02-27T16:11:37.057344",
     "exception": false,
     "start_time": "2024-02-27T16:11:37.032043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.301888,
   "end_time": "2024-02-27T16:11:37.688208",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-27T16:10:34.386320",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
